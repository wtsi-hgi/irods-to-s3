#!/usr/bin/env python

"""
Copyright (c) 2020 Genome Research Limited

Author: Christopher Harrison <ch12@sanger.ac.uk>

This program is free software: you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your
option) any later version.

This program is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
Public License for more details.

You should have received a copy of the GNU General Public License along
with this program. If not, see https://www.gnu.org/licenses/
"""

import sys
from functools import partial
from datetime import datetime

from src import arg, irods, s3, utils
from src.log import Level, Logger


class BucketNotFound(Exception):
    """ Raised when an S3 bucket doesn't exist """

class ContainsRestricted(Exception):
    """ Raised when an iRODS data object contains S3 restricted characters """

class ContainsSpecial(Exception):
    """ Raised when an iRODS data object contains S3 special characters """

class ObjectAlreadyExists(Exception):
    """ Raised when an S3 object already exists """


def _handle_exception(log:Logger, exc:Exception) -> None:
    # Log the exception, show the help text and exit non-zero
    log.error(str(exc))
    arg.help()
    sys.exit(1)


if __name__ == "__main__":
    log = Logger(Level.Warning)
    _handle = partial(_handle_exception, log)

    try:
        args = arg.parse()
    except s3.InvalidS3Bucket as exc:
        _handle(exc)
    except s3.InvalidS3URL as exc:
        _handle(exc)

    if args.verbose:
        log.level = Level.Info

    try:
        s3client = s3.S3Client(config=args.s3cfg)
    except FileNotFoundError as exc:
        _handle(exc)
    except s3.InvalidS3Config as exc:
        _handle(exc)

    bucket = args.target.bucket
    if not s3client.bucket_exists(bucket):
        if not args.make_bucket:
            _handle(BucketNotFound(f"No such bucket s3://{bucket}"))

        log.info(f"Creating bucket s3://{bucket}")
        s3client.make_bucket(bucket)

    with irods.session() as isess:
        path_to_irods = partial(irods.get_irods_object, isess)

        try:
            data_objects = irods.expand(*map(path_to_irods, args.source), recursive=args.recursive)
        except irods.NotAbsolute as exc:
            _handle(exc)
        except irods.NoSuchObject as exc:
            _handle(exc)

        try:
            for data_object in data_objects:
                s3_target = args.target / data_object.path
                log.info(f"Transferring {data_object.path} to {s3_target.url}")

                if s3_target.has_restricted:
                    message = f"iRODS data object {data_object.path} contains restricted S3 characters"

                    if not args.allow_restricted:
                        raise ContainsRestricted(message)

                    log.warning(message)

                if s3_target.has_special and args.forbid_special:
                    message = f"iRODS data object {data_object.path} contains special S3 characters"

                    if args.forbid_special:
                        raise ContainsSpecial(message)

                    log.warning(message)

                if not args.dry_run:
                    if s3client.object_exists(s3_target):
                        message = f"S3 object {s3_target.url} already exists"

                        if not args.force:
                            raise ObjectAlreadyExists(message)

                        log.info(f"{message}; replacing")
                        s3client.delete_object(s3_target)

                    start_ts = datetime.utcnow()

                    checksum = utils.hex2bytes(data_object.checksum)
                    metadata = None if args.ignore_avus else irods.metadata(data_object)

                    with data_object.open("r+") as data_stream:
                        try:
                            s3client.transfer(data_stream, s3_target, size=data_object.size,
                                                                      checksum=checksum,
                                                                      metadata=metadata)
                        except s3.ClientError as exc:
                            # Don't bail out if the transfer fails, just
                            # continue with the next data object
                            log.error(exc)
                            continue

                    tx_time = datetime.utcnow() - start_ts
                    tx_rate = data_object.size / tx_time.total_seconds()

                    message = "Complete; " \
                              f"transferred {utils.human_size(data_object.size)}B " \
                              f"at {utils.human_size(tx_rate)}B/s"

                    if checksum is not None:
                        message = f"{message}, MD5 {data_object.checksum}"

                    log.info(message)


        # NOTE The generator is evaluated lazily, so exceptions will
        # only be raised when a dodgy data object is hit, rather than
        # ahead of time (i.e., some transfers could go through before)

        except irods.CannotDescend as exc:
            _handle(exc)

        except ContainsRestricted as exc:
            _handle(exc)

        except ContainsSpecial as exc:
            _handle(exc)

        except ObjectAlreadyExists as exc:
            _handle(exc)
